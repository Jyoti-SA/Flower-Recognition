{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de Flores\n",
    "\n",
    "## Práctica Final - Visión por computador\n",
    "---\n",
    "##### Autores: Marta Gómez Macías, Braulio Vargas López\n",
    "---\n",
    "\n",
    "### Definición del problema\n",
    "\n",
    "El problema elegido consiste en extraer una serie de características del conjunto de imágenes para ser capaces de reconocer la especie de flor que es de las 17 especies posibles, mediante una imagen. Este problema consta de varias partes:\n",
    "\n",
    "1. __Extracción de las características principales de la imagen__.\n",
    "2. __Entrenar un algoritmo de aprendizaje a partir de las características extraídas__.\n",
    "\n",
    "Para esto, vamos a hacer uso de distintos descriptores, como son el descriptor *HOG*, *Shape-Context* o el descriptor de SIFT. Una vez tengamos los descriptores, vamos a crear clústers mediante el método del _vecino más cercano_ y, por último, vamos a hacer uso de un descriptor _bag of words_ para entrenar un algoritmo de aprendizaje.\n",
    "\n",
    "El descriptor _bag of words_ de OpenCV tiene un problema y es que necesita `keyPoints`, cosa que el descriptor _HOG_ no devuelve, por lo que sólo podremos hacer esta aproximación usando un descriptor SIFT, SURF o cualquiera de la clase `Features2D` de OpenCV.\n",
    "\n",
    "#### Descriptor de HOG\n",
    "\n",
    "El descriptor HOG, conocido como ___Histogram of Oriented Gradientes___ muy utilizado para la detección de objetos. Este descriptor computa un histograma de valores donde se indican la frecuencia con la que se dan las direcciones de los gradientes en pequeñas divisiones de la imagen.\n",
    "\n",
    "#### Descriptor Shape-Context\n",
    "\n",
    "\n",
    "#### Descriptor de SIFT\n",
    "\n",
    "\n",
    "\n",
    "### Enfoque de la implementación y eficiencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1: Extracción de las características principales de la imagen\n",
    "\n",
    "En este primer paso, hemos implementado una función que calcula varios tipos de descriptores de una imagen. Esta función devuelve descriptores sin clusterizar de las distintas imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_unclustered_geometric_vocabulary(images, detector_type):\n",
    "    # Create an empty vocabulary\n",
    "    vocabulary = []\n",
    "\n",
    "    if detector_type == 'SURF':\n",
    "        detector = cv2.xfeature2d.SURF_create()\n",
    "\n",
    "    elif detector_type == 'SIFT:':\n",
    "        detector = cv2.xfeature2d.SIFT_create()\n",
    "\n",
    "    elif detector_type == 'AKAZE':\n",
    "        detector = cv2.xfeature2d.AKAZE_create()\n",
    "\n",
    "    elif detector_type == 'MSD':\n",
    "        detector = cv2.xfeature2d.MSDDetector_create()\n",
    "\n",
    "    elif detector_type == 'FFD':\n",
    "        detector = cv2.xfeature2d.FastFeatureDetector_create()\n",
    "\n",
    "    else:\n",
    "        raise ValueError('Not a suitable detector')\n",
    "\n",
    "    for img in images:\n",
    "        # Detect the keypoints on the image and\n",
    "        # compute the descriptor for those keypoints\n",
    "        keypoints, descriptor = detector.detectAndCompute(img, None)\n",
    "        vocabulary.append(descriptor)\n",
    "\n",
    "    return np.array(vocabulary, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Clusterizar los descriptores obtenidos\n",
    "\n",
    "En este paso hemos implementado una función que, usando la función `kmeans` de OpenCV clusteriza los descriptores obtenidos anteriormente en $K$ clases distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementación del modelo de aprendizaje\n",
    "Para realizar los modelos de aprendizaje, hemos usado la librería `ml` de _OpenCV_, más concretamente hemos entrenado un modelo _Support Vector Machine_ y un modelo _Random Forest_. A la hora de dividir los datos en conjunto de training y test, hemos procurado que haya, al menos, una imagen de cada clase en ambos conjuntos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_train_subset():\n",
    "    subset = []\n",
    "    for i in range(0,1360,80):\n",
    "        subset += np.random.randint(low=i,high=i+80,size=4).tolist()\n",
    "\n",
    "    return np.array(subset)\n",
    "\n",
    "\n",
    "test_mask = create_train_subset()\n",
    "aux = np.arange(1360)\n",
    "training_mask = np.in1d(aux, test_mask) * 1\n",
    "training_mask = np.where(training_mask == 0)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
